#!/usr/bin/env python3
"""
Script to quantize FC_AE anomaly detection model using Neural Compressor
with ToyADMOS dataset from HuggingFace.

This script:
1. Downloads the ToyADMOS validation set from HuggingFace
2. Preprocesses the audio data to mel spectrograms (640 features)
3. Quantizes the ad_raw.onnx model to 8-bit using QOperator format
"""

import os
import numpy as np
import librosa
import torch
import glob
from datasets import load_dataset
from neural_compressor import quantization, PostTrainingQuantConfig
from neural_compressor.config import AccuracyCriterion

def load_toyadmos_validation_data(num_samples_per_machine=50):
    """
    Load ToyADMOS dataset from HuggingFace for FC_AE quantization
    Returns preprocessed mel spectrograms for validation
    """
    print("Loading ToyADMOS dataset from HuggingFace...")
    
    # Load the dataset - using a subset for efficient quantization
    try:
        dataset = load_dataset("YoshiKKawai/ToyADMOS", split="test")
        print(f"Loaded {len(dataset)} samples from ToyADMOS test set")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        print("Generating synthetic data for testing...")
        return generate_synthetic_toyadmos_data(num_samples_per_machine * 4)
    
    # Filter and limit samples per machine type
    machine_types = ["car", "ToyConveyor", "ToyCar", "ToyTrain"]
    validation_data = []
    
    for machine_type in machine_types:
        machine_samples = [sample for sample in dataset if sample.get("machine_type") == machine_type]
        # Limit samples to avoid excessive memory usage
        limited_samples = machine_samples[:num_samples_per_machine]
        validation_data.extend(limited_samples)
        print(f"Added {len(limited_samples)} samples for {machine_type}")
    
    if not validation_data:
        print("No samples found, using all available data...")
        validation_data = list(dataset)[:num_samples_per_machine * 4]
    
    print(f"Total validation samples: {len(validation_data)}")
    
    # Preprocess audio to mel spectrograms
    mel_spectrograms = []
    for idx, sample in enumerate(validation_data):
        if idx % 20 == 0:
            print(f"Processing sample {idx+1}/{len(validation_data)}")
        
        try:
            # Get audio data
            audio = sample["audio"]["array"]
            sr = sample["audio"]["sampling_rate"]
            
            # Convert to mel spectrogram following toycar_dset.py format
            # Expected output: [217, 640] where 640 = 128 mel bands √ó 5 frames
            mel_spec_2d = audio_to_mel_spectrogram_toycar_format(audio, sr)
            
            mel_spectrograms.append(mel_spec_2d.astype(np.float32))
            
        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            # Use synthetic data for failed samples - correct shape [217, 640]
            synthetic_sample = np.random.randn(217, 640).astype(np.float32)
            # Apply toycar_dset.py normalization
            std = 12.0590
            mean = -28.0861
            synthetic_sample = (synthetic_sample - mean) / std
            mel_spectrograms.append(synthetic_sample)
    
    return np.array(mel_spectrograms)

def generate_synthetic_toyadmos_data(num_samples=200):
    """Generate synthetic mel spectrogram data for testing - shape [217, 640]"""
    print(f"Generating {num_samples} synthetic mel spectrograms with shape [217, 640]...")
    
    synthetic_data = []
    for i in range(num_samples):
        # Generate data matching toycar_dset.py format: [217, 640]
        # 217 time frames, 640 features (128 mel bands √ó 5 frames)
        mel_spec_2d = np.random.randn(217, 640).astype(np.float32)
        
        # Add temporal patterns typical of machine sounds
        for t in range(217):
            # Add periodic patterns across time
            freq_pattern = np.sin(2 * np.pi * t / 217) * 0.3
            amplitude_mod = np.cos(2 * np.pi * t / 50) * 0.2
            
            # Apply patterns to features with some variation
            mel_spec_2d[t] += freq_pattern * np.random.randn(640) * 0.1
            mel_spec_2d[t] += amplitude_mod
        
        # Normalize following toycar_dset.py normalization
        std = 12.0590
        mean = -28.0861
        mel_spec_2d = (mel_spec_2d - mean) / std
        
        synthetic_data.append(mel_spec_2d)
    
    return np.array(synthetic_data)

def audio_to_mel_spectrogram_toycar_format(audio, sr):
    """
    Convert audio to mel spectrogram following toycar_dset.py format
    Returns: shape [217, 640] where 640 = 128 mel bands √ó 5 frames
    """
    try:
        import torchaudio
        import torch
        import sys
        
        # Ensure audio is 1D and convert to torch tensor
        if len(audio.shape) > 1:
            audio = audio.mean(axis=1)
        
        # Convert to torch tensor and add batch dimension
        waveform = torch.from_numpy(audio).unsqueeze(0).float()
        
        # Follow toycar_dset.py preprocessing exactly
        # Crop to specific length if needed (this matches the original preprocessing)
        target_length = 144320 - 31680  # Length used in toycar_dset.py
        if waveform.shape[1] > target_length:
            waveform = waveform[:, :target_length]
        elif waveform.shape[1] < target_length:
            # Pad with zeros
            padding = target_length - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, padding))
        
        # Mel spectrogram transform following toycar_dset.py
        mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr,
            n_mels=128,
            n_fft=1024,
            win_length=1024,
            hop_length=512,
            power=2,
            mel_scale='slaney',
            norm='slaney',
            pad_mode='constant',
            center=True
        )
        
        mel_spectrogram = mel_transform(waveform)
        
        # Convert to log scale
        log_mel_energies = 20.0 / 2.0 * torch.log10(mel_spectrogram + sys.float_info.epsilon)
        
        # Reshape following toycar_dset.py: (t, 128) -> sliding window -> (time_frames, 640)
        f1 = log_mel_energies.squeeze().permute(1, 0)  # (t, 128)
        
        # Create sliding window of 5 frames as in toycar_dset.py
        f2 = []
        for t in range(f1.shape[0] - 4):  # -4 because we need 5 consecutive frames
            f2.append(f1[t:t+5].flatten())  # Flatten 5√ó128 -> 640
        
        if len(f2) == 0:
            # Fallback: create at least one frame
            f2.append(f1[:5].flatten() if f1.shape[0] >= 5 else torch.zeros(640))
        
        f3 = torch.stack(f2)  # Shape: (time_frames, 640)
        
        # Normalize following toycar_dset.py values
        std = 12.0590
        mean = -28.0861
        f3 = (f3 - mean) / std
        
        # Ensure we have exactly 217 time frames
        if f3.shape[0] > 217:
            f3 = f3[:217]
        elif f3.shape[0] < 217:
            # Pad with the last frame repeated
            padding_needed = 217 - f3.shape[0]
            if f3.shape[0] > 0:
                last_frame = f3[-1].unsqueeze(0)
                padding = last_frame.repeat(padding_needed, 1)
                f3 = torch.cat([f3, padding], dim=0)
            else:
                f3 = torch.zeros(217, 640)
        
        return f3.numpy()
        
    except Exception as e:
        print(f"Error in toycar format conversion: {e}")
        # Fallback: return synthetic data with correct shape
        return np.random.randn(217, 640).astype(np.float32)

def audio_to_mel_spectrogram(audio, sr, n_mels=64, hop_length=512, n_fft=1024):
    """Convert audio to mel spectrogram (legacy function for compatibility)"""
    try:
        # Ensure audio is 1D
        if len(audio.shape) > 1:
            audio = audio.mean(axis=1)
        
        # Compute mel spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=audio, 
            sr=sr, 
            n_mels=n_mels,
            hop_length=hop_length,
            n_fft=n_fft
        )
        
        # Convert to log scale
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        
        return log_mel_spec
        
    except Exception as e:
        print(f"Error in mel spectrogram conversion: {e}")
        # Return random data if conversion fails
        return np.random.randn(n_mels, audio.shape[0] // hop_length + 1)

class ToyADMOSDataLoader:
    """Custom DataLoader for ToyADMOS data compatible with neural compressor"""
    
    def __init__(self, data, batch_size=1):
        self.data = data
        self.batch_size = batch_size
        self.length = len(data)
        self.index = 0
    
    def __iter__(self):
        self.index = 0
        return self
    
    def __next__(self):
        if self.index >= self.length:
            raise StopIteration
        
        batch_data = []
        for _ in range(min(self.batch_size, self.length - self.index)):
            batch_data.append(self.data[self.index])
            self.index += 1
        
        # Return as tuple (input, label) - for autoencoder, input == target
        batch_array = np.array(batch_data)
        return batch_array, batch_array
    
    def __len__(self):
        return (self.length + self.batch_size - 1) // self.batch_size

class SingleSampleDataLoader:
    """DataLoader that yields individual samples of shape [217, 640] for calibration"""
    
    def __init__(self, sample, num_iterations=50, batch_size=1):
        self.sample = sample  # Shape should be [217, 640]
        self.num_iterations = num_iterations
        self.batch_size = batch_size  # Required by Neural Compressor
        self.current_iteration = 0
    
    def __iter__(self):
        self.current_iteration = 0
        return self
    
    def __next__(self):
        if self.current_iteration >= self.num_iterations:
            raise StopIteration
        
        self.current_iteration += 1
        # Return individual sample, not batched
        # For autoencoder: input == target
        return self.sample, self.sample
    
    def __len__(self):
        return self.num_iterations

def load_real_toyadmos_sample():
    """Load a single real ToyADMOS sample and preprocess to [217, 640]"""
    import torchaudio
    import glob
    
    # Check common paths where ToyADMOS might be stored
    possible_paths = [
        "/home/laquizon/lawrence-workspace/data/ToyCar/",
        "/home/raimarc/lawrence-workspace/data/ToyCar/train/",
        "./data/ToyCar/",
        "../data/ToyCar/",
        "../../data/ToyCar/"
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            print(f"Found ToyADMOS data at: {path}")
            # Look for .wav files
            wav_files = glob.glob(os.path.join(path, "**/*.wav"), recursive=True)
            if wav_files:
                print(f"Found {len(wav_files)} WAV files")
                # Load the first one
                try:
                    audio_file = wav_files[0]
                    print(f"Loading: {audio_file}")
                    
                    # Load with torchaudio
                    waveform, sample_rate = torchaudio.load(audio_file)
                    print(f"Loaded audio: shape={waveform.shape}, sr={sample_rate}")
                    
                    # Convert to numpy and process
                    audio_array = waveform.numpy().squeeze()
                    return audio_to_mel_spectrogram_toycar_format(audio_array, sample_rate)
                    
                except Exception as e:
                    print(f"Error loading {audio_file}: {e}")
                    continue
    
    # Try HuggingFace datasets as fallback
    try:
        print("Trying to load from HuggingFace...")
        dataset = load_dataset("floras/ToyADMOS", split="test", streaming=True)
        sample = next(iter(dataset))
        print("Successfully loaded ToyADMOS sample from HuggingFace")
        
        audio = sample["audio"]["array"]
        sr = sample["audio"]["sampling_rate"]
        return audio_to_mel_spectrogram_toycar_format(audio, sr)
        
    except Exception as e:
        print(f"Error loading from HuggingFace: {e}")
    
    print("Cannot load real ToyADMOS data. Using synthetic sample.")
    # Generate ONE sample with correct shape
    mel_spec_2d = np.random.randn(217, 640).astype(np.float32)
    
    # Add realistic machine sound patterns
    for t in range(217):
        freq_pattern = np.sin(2 * np.pi * t / 217) * 0.3
        amplitude_mod = np.cos(2 * np.pi * t / 50) * 0.2
        mel_spec_2d[t] += freq_pattern * np.random.randn(640) * 0.1
        mel_spec_2d[t] += amplitude_mod
    
    # Normalize
    std = 12.0590
    mean = -28.0861
    mel_spec_2d = (mel_spec_2d - mean) / std
    
    return mel_spec_2d

def quantize_fc_ae_toyadmos(
    model_path="onnx_models/ad_raw.onnx",
    output_path="onnx_models/ad_quantized_int8.onnx",
    num_calibration_iterations=50,
    batch_size=1
):
    """
    Quantize FC_AE model for ToyADMOS anomaly detection using neural compressor
    
    Args:
        model_path: Path to the input ONNX model
        output_path: Path to save the quantized model
        num_calibration_iterations: Number of calibration iterations (using same sample)
        batch_size: Batch size (should be 1 for individual samples)
    """
    print("=" * 60)
    print("FC_AE ToyADMOS Model Quantization with Neural Compressor")
    print("=" * 60)
    
    # Check if model exists
    if not os.path.exists(model_path):
        print(f"Error: Model file {model_path} not found!")
        print("Please ensure the ONNX model exists before quantization.")
        return None
    
    # Load a single real ToyADMOS sample
    print(f"\nLoading ToyADMOS sample for calibration...")
    real_sample = load_real_toyadmos_sample()
    
    if real_sample is None:
        print("Error: No validation data loaded!")
        return None
    
    print(f"Loaded sample with shape: {real_sample.shape}")
    
    # Create data loader that yields individual samples
    dataloader = SingleSampleDataLoader(real_sample, num_iterations=num_calibration_iterations, batch_size=batch_size)
    
    # Set up quantization configuration
    accuracy_criterion = AccuracyCriterion()
    accuracy_criterion.relative = 0.02  # 2% accuracy tolerance
    
    config = PostTrainingQuantConfig(
        approach="static",
        quant_level=1,  # 8-bit quantization
        quant_format="QOperator"  # QOperator format as requested
    )
    
    print(f"\nStarting quantization...")
    print(f"Model: {model_path}")
    print(f"Approach: static")
    print(f"Quant Level: 1 (8-bit)")
    print(f"Quant Format: QOperator")
    print(f"Calibration iterations: {num_calibration_iterations}")
    print(f"Sample shape: {real_sample.shape}")
    
    try:
        # Perform quantization
        quantized_model = quantization.fit(
            model=model_path,
            conf=config,
            calib_dataloader=dataloader,
            accuracy_criterion=accuracy_criterion
        )
        
        # Save quantized model
        quantized_model.save(output_path)
        
        # Check file sizes
        original_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
        quantized_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        compression_ratio = original_size / quantized_size
        
        print("\n" + "=" * 60)
        print("QUANTIZATION COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print(f"Original model: {model_path}")
        print(f"Quantized model: {output_path}")
        print(f"Original size: {original_size:.2f} MB")
        print(f"Quantized size: {quantized_size:.2f} MB")
        print(f"Compression ratio: {compression_ratio:.2f}x")
        print(f"Size reduction: {((original_size - quantized_size) / original_size * 100):.1f}%")
        
        return quantized_model
        
    except Exception as e:
        print(f"\nError during quantization: {e}")
        print("This might be due to:")
        print("- Incompatible ONNX model format")
        print("- Missing neural-compressor dependencies")
        print("- Insufficient calibration data")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # Run quantization with improved approach
    quantized_model = quantize_fc_ae_toyadmos(
        model_path="onnx_models/ad_raw.onnx",
        output_path="onnx_models/ad_quantized_int8.onnx",
        num_calibration_iterations=50,  # Use single sample multiple times
        batch_size=1
    )
    
    if quantized_model:
        print("\n‚úÖ FC_AE ToyADMOS quantization completed successfully!")
        print("üéØ 8-bit QOperator quantized model saved to: onnx_models/ad_quantized_int8.onnx")
        print("üí° Used individual samples of shape [217, 640] as required by the model")
    else:
        print("\n‚ùå Quantization failed. Please check the error messages above.")
